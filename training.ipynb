{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import ast\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simple_custom_taxi_env # the custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_limit = 5000\n",
    "action_size = 6\n",
    "Envs = [simple_custom_taxi_env.SimpleTaxiEnv(fuel_limit = 5000)] # modified from gym Taxi-v3\n",
    "def sign(x):\n",
    "    return np.sign(x)\n",
    "\n",
    "A = {\"Move South\":0, \"Move North\":1, \"Move East\":2, \"Move West\":3, \"Pick Up\":4, \"Drop Off\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_table_learning():\n",
    "    def __init__(self, grid_size = 7, obstacle_count = 0, Env_index=0):\n",
    "        self.Q_table = {}\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.6\n",
    "        self.epsilon_end = 0.1\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_decay = 0.99995\n",
    "        #self.grid_size = grid_size\n",
    "        #self.fuel_limit = fuel_limit\n",
    "        self.env = Envs[Env_index]\n",
    "\n",
    "        #for testing\n",
    "        self.global_current_passenger_picked = False\n",
    "        self.global_current_state = None\n",
    "        self.global_current_obs = None\n",
    "        self.global_action = None\n",
    "        self.test_table = {}\n",
    "\n",
    "        self.target_counter = 0\n",
    "        self.targets = []\n",
    "    \n",
    "    def get_state(self, current_state, next_obs, action, current_passenger_picked):\n",
    "        \"\"\"\n",
    "        Convert observations into a structured state for Q-learning.\n",
    "        Tracks passenger pickup and drop-off correctly.\n",
    "        \"\"\"\n",
    "        taxi_row, taxi_col, S1x, S1y, S2x, S2y, S3x, S3y, S4x, S4y, obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = next_obs\n",
    "        # Compute directions towards the stations\n",
    "        if len(self.targets) == 0:\n",
    "            self.targets.append((S1x, S1y))\n",
    "            self.targets.append((S2x, S2y))\n",
    "            self.targets.append((S3x, S3y))\n",
    "            self.targets.append((S4x, S4y))\n",
    "        on_station = (taxi_row, taxi_col) in [(S1x, S1y), (S2x, S2y), (S3x, S3y), (S4x, S4y)]\n",
    "\n",
    "        reached = False\n",
    "\n",
    "        if (taxi_row, taxi_col) == self.targets[self.target_counter]:\n",
    "            reached = True\n",
    "            \n",
    "        tar_dir = (np.sign(self.targets[self.target_counter][0] - taxi_row), np.sign(self.targets[self.target_counter][1] - taxi_col))\n",
    "        passenger_picked = current_passenger_picked\n",
    "        \n",
    "        if action == 4 and passenger_look and not current_passenger_picked and on_station:\n",
    "            passenger_picked = True  # Passenger is now inside the taxi\n",
    "\n",
    "        state = (\n",
    "            tar_dir[0], tar_dir[1], \n",
    "            passenger_look, destination_look,\n",
    "            obstacle_north, obstacle_south, obstacle_east, obstacle_west, action,\n",
    "            passenger_picked\n",
    "        )\n",
    "       \n",
    "        return state, on_station, reached\n",
    "\n",
    "    def Train(self, total_episodes):\n",
    "        rewards_per_episode = []\n",
    "        steps_to_end = []\n",
    "        passenger_pick_per_episode = []\n",
    "        epsilon = self.epsilon_start\n",
    "        for episode in range(total_episodes):\n",
    "            current_obs, info = self.env.reset()\n",
    "            self.targets.clear()\n",
    "            self.target_counter = 0\n",
    "            #print(self.env.grid_size)\n",
    "            done = False\n",
    "            pre_action = -1\n",
    "            stepCnt = 0\n",
    "            total_reward_episode = 0\n",
    "            current_passenger_picked = False  # Track if the taxi has the passenger\n",
    "            cur_state, _ , _= self.get_state(None, current_obs, None, current_passenger_picked)\n",
    "            while not done:\n",
    "                if cur_state not in self.Q_table:\n",
    "                    self.Q_table[cur_state] = np.zeros(action_size)\n",
    "                    #self.q_init(cur_state)\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = np.random.choice([0,1,2,3,4,5])  # Random action\n",
    "                else:\n",
    "                    action = np.argmax(self.Q_table[cur_state])\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "                next_state, on_station, reached_target = self.get_state(cur_state, next_obs, action, current_passenger_picked)\n",
    "                passenger_picked = next_state[-1]\n",
    "                stepCnt += 1\n",
    "                # Reward shaping\n",
    "                shaped_reward = 0\n",
    "                shaped_reward -= 0.1\n",
    "                \n",
    "                if action == 4 and not current_passenger_picked and passenger_picked:\n",
    "                    if on_station:\n",
    "                        passenger_pick_per_episode.append(stepCnt)\n",
    "                        shaped_reward += 80\n",
    "                        #print('picked')\n",
    "\n",
    "                if reached_target:\n",
    "                    shaped_reward += 5\n",
    "\n",
    "                if (pre_action == 0 and action == 1) or (pre_action == 1 and action == 0) or (pre_action == 2 and action == 3) or (pre_action == 3 and action == 2):\n",
    "                    shaped_reward -= 0.5\n",
    "                    \n",
    "                if on_station and current_passenger_picked and cur_state[3] == 1 and action == 5 and done:\n",
    "                    shaped_reward += 500\n",
    "                    #print(\"done\")\n",
    "                \n",
    "                if not on_station and action == 4 and not passenger_picked:\n",
    "                    shaped_reward -= 5\n",
    "\n",
    "                if (not current_passenger_picked or not cur_state[3] or not on_station) and action == 5 :\n",
    "                    shaped_reward -= 15\n",
    "                #don't hit the wall plz\n",
    "                if (cur_state[4] == 1 and action == 1) or (cur_state[5] == 1 and action == 0) or (cur_state[6] == 1 and action == 2) or (cur_state[7] == 1 and action == 3):\n",
    "                    shaped_reward -= 10\n",
    "\n",
    "                ctarget_diry, ctarget_dirx = cur_state[0:2]\n",
    "                if ctarget_diry > 0 and action == 0:\n",
    "                    shaped_reward += 0.04\n",
    "                if ctarget_diry < 0 and action == 1:\n",
    "                    shaped_reward += 0.04\n",
    "                if ctarget_dirx > 0 and action == 2:\n",
    "                    shaped_reward += 0.04\n",
    "                if ctarget_dirx < 0 and action == 3:\n",
    "                    shaped_reward += 0.04\n",
    "\n",
    "                if reached_target:\n",
    "                    self.target_counter = (self.target_counter+1)%4\n",
    "\n",
    "                reward += shaped_reward\n",
    "                total_reward_episode += reward\n",
    "\n",
    "                # Update Q-table\n",
    "                if next_state not in self.Q_table:\n",
    "                    self.Q_table[next_state] = np.zeros(action_size)\n",
    "                    #self.q_init(next_state)\n",
    "                \n",
    "                self.Q_table[cur_state][action] += self.alpha * (reward + self.gamma * np.max(self.Q_table[next_state]) - self.Q_table[cur_state][action])\n",
    "\n",
    "                cur_state = next_state\n",
    "                current_obs = next_obs  # Move to the next state\n",
    "                current_passenger_picked = passenger_picked\n",
    "                stepCnt += 1\n",
    "                pre_action = action\n",
    "\n",
    "            rewards_per_episode.append(total_reward_episode)\n",
    "            steps_to_end.append(stepCnt)\n",
    "            epsilon = max(self.epsilon_end, epsilon * self.epsilon_decay)\n",
    "\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                avg_rewards = np.mean(rewards_per_episode[-100:])\n",
    "                avg_steps = np.mean(steps_to_end[-100:])\n",
    "                avg_passenger_picked = np.mean(passenger_pick_per_episode[-100:])\n",
    "                print(f\"ðŸš€ Episode {episode + 1}/{total_episodes}, Epsilon: {epsilon:.3f}\")\n",
    "                print(f\"Average Reward: {avg_rewards:.2f}\")\n",
    "                print(f\"Average passenger: {avg_passenger_picked:.2f}\")\n",
    "                print(f\"Steps to end: {avg_steps:.2f}\")\n",
    "            \n",
    "        self.extractTable()\n",
    "\n",
    "    def isreset(self,current_obs, obs):\n",
    "        return current_obs == None or current_obs[2:10] != obs[2:10]\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if self.isreset(self.global_current_obs, obs):\n",
    "            self.global_current_passenger_picked = False\n",
    "        \n",
    "        next_state,_ = self.get_state(self.global_current_state, obs, self.global_action, self.global_current_passenger_picked)\n",
    "        if next_state not in self.test_table:\n",
    "            self.global_action = np.random.choice([0,1,2,3,4,5])\n",
    "        else:\n",
    "            self.global_action = np.argmax(self.test_table[next_state])\n",
    "            #if next_state[-1] == True:\n",
    "                #print('picked')\n",
    "\n",
    "        self.global_current_obs = obs\n",
    "        self.global_current_state = next_state\n",
    "        self.global_current_passenger_picked = self.global_current_state[-1]\n",
    "        return self.global_action\n",
    "\n",
    "\n",
    "    def Test(self, total_episodes=100):\n",
    "        rewards_per_episode = []\n",
    "        steps_to_end = []\n",
    "        actions_cnt = [0,0,0,0,0,0]\n",
    "\n",
    "        self.test_table = self.loadTable()\n",
    "        for episode in range(total_episodes):\n",
    "            obs, info = self.env.reset()\n",
    "            done = False\n",
    "            total_reward_episode = 0\n",
    "            stepCnt = 0\n",
    "            while not done:\n",
    "                action = self.get_action(obs)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                total_reward_episode += reward\n",
    "                stepCnt += 1\n",
    "            rewards_per_episode.append(total_reward_episode)\n",
    "            steps_to_end.append(stepCnt)\n",
    "            #epsilon = max(self.epsilon_end, epsilon * self.epsilon_decay)\n",
    "\n",
    "        return rewards_per_episode, steps_to_end\n",
    "        \n",
    "    def extractTable(self):\n",
    "        with open('q_table.csv', 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for key, value in self.Q_table.items():\n",
    "                writer.writerow([key] + list(value))  # Store key as first column, values as rest\n",
    "\n",
    "    def loadTable(self):\n",
    "        with open('q_table.csv', 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            Q_table = {eval(row[0]): list(map(float, row[1:])) for row in reader}\n",
    "        return Q_table\n",
    "    \n",
    "    def run_env(self):\n",
    "        self.env.run_agent()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_learning_wrapper = Q_table_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Episode 100/40000, Epsilon: 0.995\n",
      "Average Reward: -26405.11\n",
      "Average passenger: 621.62\n",
      "Steps to end: 6108.88\n",
      "ðŸš€ Episode 200/40000, Epsilon: 0.990\n",
      "Average Reward: -28063.56\n",
      "Average passenger: 736.20\n",
      "Steps to end: 6463.66\n",
      "ðŸš€ Episode 300/40000, Epsilon: 0.985\n",
      "Average Reward: -26210.60\n",
      "Average passenger: 775.00\n",
      "Steps to end: 6151.60\n",
      "ðŸš€ Episode 400/40000, Epsilon: 0.980\n",
      "Average Reward: -28027.65\n",
      "Average passenger: 930.96\n",
      "Steps to end: 6545.26\n",
      "ðŸš€ Episode 500/40000, Epsilon: 0.975\n",
      "Average Reward: -26950.74\n",
      "Average passenger: 667.92\n",
      "Steps to end: 6312.90\n",
      "ðŸš€ Episode 600/40000, Epsilon: 0.970\n",
      "Average Reward: -25397.02\n",
      "Average passenger: 724.54\n",
      "Steps to end: 6095.46\n",
      "ðŸš€ Episode 700/40000, Epsilon: 0.966\n",
      "Average Reward: -25523.95\n",
      "Average passenger: 766.10\n",
      "Steps to end: 6063.50\n",
      "ðŸš€ Episode 800/40000, Epsilon: 0.961\n",
      "Average Reward: -23820.95\n",
      "Average passenger: 732.16\n",
      "Steps to end: 5771.34\n",
      "ðŸš€ Episode 900/40000, Epsilon: 0.956\n",
      "Average Reward: -25487.31\n",
      "Average passenger: 522.44\n",
      "Steps to end: 6211.48\n",
      "ðŸš€ Episode 1000/40000, Epsilon: 0.951\n",
      "Average Reward: -23687.94\n",
      "Average passenger: 468.18\n",
      "Steps to end: 5814.28\n",
      "ðŸš€ Episode 1100/40000, Epsilon: 0.946\n",
      "Average Reward: -21915.23\n",
      "Average passenger: 641.24\n",
      "Steps to end: 5450.30\n",
      "ðŸš€ Episode 1200/40000, Epsilon: 0.942\n",
      "Average Reward: -24039.90\n",
      "Average passenger: 666.66\n",
      "Steps to end: 5993.60\n",
      "ðŸš€ Episode 1300/40000, Epsilon: 0.937\n",
      "Average Reward: -23143.50\n",
      "Average passenger: 629.84\n",
      "Steps to end: 5836.28\n",
      "ðŸš€ Episode 1400/40000, Epsilon: 0.932\n",
      "Average Reward: -22025.73\n",
      "Average passenger: 797.60\n",
      "Steps to end: 5594.10\n",
      "ðŸš€ Episode 1500/40000, Epsilon: 0.928\n",
      "Average Reward: -20826.14\n",
      "Average passenger: 755.16\n",
      "Steps to end: 5318.80\n",
      "ðŸš€ Episode 1600/40000, Epsilon: 0.923\n",
      "Average Reward: -21624.29\n",
      "Average passenger: 744.74\n",
      "Steps to end: 5533.74\n",
      "ðŸš€ Episode 1700/40000, Epsilon: 0.919\n",
      "Average Reward: -18844.88\n",
      "Average passenger: 675.24\n",
      "Steps to end: 4842.46\n",
      "ðŸš€ Episode 1800/40000, Epsilon: 0.914\n",
      "Average Reward: -21825.05\n",
      "Average passenger: 644.78\n",
      "Steps to end: 5699.68\n",
      "ðŸš€ Episode 1900/40000, Epsilon: 0.909\n",
      "Average Reward: -19164.57\n",
      "Average passenger: 756.72\n",
      "Steps to end: 5039.24\n",
      "ðŸš€ Episode 2000/40000, Epsilon: 0.905\n",
      "Average Reward: -20710.36\n",
      "Average passenger: 762.92\n",
      "Steps to end: 5448.74\n",
      "ðŸš€ Episode 2100/40000, Epsilon: 0.900\n",
      "Average Reward: -19450.90\n",
      "Average passenger: 678.48\n",
      "Steps to end: 5137.94\n",
      "ðŸš€ Episode 2200/40000, Epsilon: 0.896\n",
      "Average Reward: -22243.52\n",
      "Average passenger: 698.78\n",
      "Steps to end: 5877.00\n",
      "ðŸš€ Episode 2300/40000, Epsilon: 0.891\n",
      "Average Reward: -19617.89\n",
      "Average passenger: 576.24\n",
      "Steps to end: 5028.46\n",
      "ðŸš€ Episode 2400/40000, Epsilon: 0.887\n",
      "Average Reward: -18070.30\n",
      "Average passenger: 545.42\n",
      "Steps to end: 4908.78\n",
      "ðŸš€ Episode 2500/40000, Epsilon: 0.882\n",
      "Average Reward: -17638.71\n",
      "Average passenger: 762.22\n",
      "Steps to end: 4818.80\n",
      "ðŸš€ Episode 2600/40000, Epsilon: 0.878\n",
      "Average Reward: -16157.75\n",
      "Average passenger: 670.68\n",
      "Steps to end: 4455.78\n",
      "ðŸš€ Episode 2700/40000, Epsilon: 0.874\n",
      "Average Reward: -19006.33\n",
      "Average passenger: 889.92\n",
      "Steps to end: 5017.10\n",
      "ðŸš€ Episode 2800/40000, Epsilon: 0.869\n",
      "Average Reward: -18719.18\n",
      "Average passenger: 684.36\n",
      "Steps to end: 5276.64\n",
      "ðŸš€ Episode 2900/40000, Epsilon: 0.865\n",
      "Average Reward: -17437.72\n",
      "Average passenger: 315.40\n",
      "Steps to end: 4937.54\n",
      "ðŸš€ Episode 3000/40000, Epsilon: 0.861\n",
      "Average Reward: -13594.71\n",
      "Average passenger: 574.20\n",
      "Steps to end: 3912.14\n",
      "ðŸš€ Episode 3100/40000, Epsilon: 0.856\n",
      "Average Reward: -16662.43\n",
      "Average passenger: 703.42\n",
      "Steps to end: 4732.32\n",
      "ðŸš€ Episode 3200/40000, Epsilon: 0.852\n",
      "Average Reward: -19016.19\n",
      "Average passenger: 611.00\n",
      "Steps to end: 5369.74\n",
      "ðŸš€ Episode 3300/40000, Epsilon: 0.848\n",
      "Average Reward: -17792.48\n",
      "Average passenger: 395.76\n",
      "Steps to end: 5098.24\n",
      "ðŸš€ Episode 3400/40000, Epsilon: 0.844\n",
      "Average Reward: -18061.93\n",
      "Average passenger: 664.46\n",
      "Steps to end: 5246.42\n",
      "ðŸš€ Episode 3500/40000, Epsilon: 0.839\n",
      "Average Reward: -15867.84\n",
      "Average passenger: 383.16\n",
      "Steps to end: 4711.20\n",
      "ðŸš€ Episode 3600/40000, Epsilon: 0.835\n",
      "Average Reward: -16868.88\n",
      "Average passenger: 632.58\n",
      "Steps to end: 4988.98\n",
      "ðŸš€ Episode 3700/40000, Epsilon: 0.831\n",
      "Average Reward: -17396.94\n",
      "Average passenger: 575.82\n",
      "Steps to end: 5169.70\n",
      "ðŸš€ Episode 3800/40000, Epsilon: 0.827\n",
      "Average Reward: -17185.03\n",
      "Average passenger: 861.50\n",
      "Steps to end: 5105.88\n",
      "ðŸš€ Episode 3900/40000, Epsilon: 0.823\n",
      "Average Reward: -14012.70\n",
      "Average passenger: 445.68\n",
      "Steps to end: 4228.24\n",
      "ðŸš€ Episode 4000/40000, Epsilon: 0.819\n",
      "Average Reward: -14532.70\n",
      "Average passenger: 580.12\n",
      "Steps to end: 4420.62\n",
      "ðŸš€ Episode 4100/40000, Epsilon: 0.815\n",
      "Average Reward: -14001.76\n",
      "Average passenger: 517.08\n",
      "Steps to end: 4268.10\n",
      "ðŸš€ Episode 4200/40000, Epsilon: 0.811\n",
      "Average Reward: -16661.00\n",
      "Average passenger: 575.06\n",
      "Steps to end: 5089.96\n",
      "ðŸš€ Episode 4300/40000, Epsilon: 0.807\n",
      "Average Reward: -14554.20\n",
      "Average passenger: 519.14\n",
      "Steps to end: 4524.44\n",
      "ðŸš€ Episode 4400/40000, Epsilon: 0.803\n",
      "Average Reward: -12493.79\n",
      "Average passenger: 557.32\n",
      "Steps to end: 3898.34\n",
      "ðŸš€ Episode 4500/40000, Epsilon: 0.799\n",
      "Average Reward: -12954.74\n",
      "Average passenger: 430.96\n",
      "Steps to end: 4099.44\n",
      "ðŸš€ Episode 4600/40000, Epsilon: 0.795\n",
      "Average Reward: -14403.92\n",
      "Average passenger: 363.78\n",
      "Steps to end: 4583.94\n",
      "ðŸš€ Episode 4700/40000, Epsilon: 0.791\n",
      "Average Reward: -13249.19\n",
      "Average passenger: 461.74\n",
      "Steps to end: 4171.30\n",
      "ðŸš€ Episode 4800/40000, Epsilon: 0.787\n",
      "Average Reward: -13023.61\n",
      "Average passenger: 581.52\n",
      "Steps to end: 4109.78\n",
      "ðŸš€ Episode 4900/40000, Epsilon: 0.783\n",
      "Average Reward: -14843.29\n",
      "Average passenger: 517.40\n",
      "Steps to end: 4665.08\n",
      "ðŸš€ Episode 5000/40000, Epsilon: 0.779\n",
      "Average Reward: -11750.93\n",
      "Average passenger: 666.78\n",
      "Steps to end: 3848.78\n",
      "ðŸš€ Episode 5100/40000, Epsilon: 0.775\n",
      "Average Reward: -12467.46\n",
      "Average passenger: 547.88\n",
      "Steps to end: 4049.56\n",
      "ðŸš€ Episode 5200/40000, Epsilon: 0.771\n",
      "Average Reward: -9325.99\n",
      "Average passenger: 583.88\n",
      "Steps to end: 3124.72\n",
      "ðŸš€ Episode 5300/40000, Epsilon: 0.767\n",
      "Average Reward: -11813.84\n",
      "Average passenger: 507.04\n",
      "Steps to end: 3959.62\n",
      "ðŸš€ Episode 5400/40000, Epsilon: 0.763\n",
      "Average Reward: -14220.89\n",
      "Average passenger: 598.50\n",
      "Steps to end: 4586.26\n",
      "ðŸš€ Episode 5500/40000, Epsilon: 0.760\n",
      "Average Reward: -12083.99\n",
      "Average passenger: 585.42\n",
      "Steps to end: 4015.56\n",
      "ðŸš€ Episode 5600/40000, Epsilon: 0.756\n",
      "Average Reward: -11722.75\n",
      "Average passenger: 809.36\n",
      "Steps to end: 3979.76\n",
      "ðŸš€ Episode 5700/40000, Epsilon: 0.752\n",
      "Average Reward: -9781.44\n",
      "Average passenger: 575.36\n",
      "Steps to end: 3301.44\n",
      "ðŸš€ Episode 5800/40000, Epsilon: 0.748\n",
      "Average Reward: -13933.38\n",
      "Average passenger: 371.60\n",
      "Steps to end: 4745.54\n",
      "ðŸš€ Episode 5900/40000, Epsilon: 0.745\n",
      "Average Reward: -10068.45\n",
      "Average passenger: 432.30\n",
      "Steps to end: 3497.50\n",
      "ðŸš€ Episode 6000/40000, Epsilon: 0.741\n",
      "Average Reward: -11562.39\n",
      "Average passenger: 591.64\n",
      "Steps to end: 3881.12\n",
      "ðŸš€ Episode 6100/40000, Epsilon: 0.737\n",
      "Average Reward: -11763.12\n",
      "Average passenger: 650.80\n",
      "Steps to end: 4006.06\n",
      "ðŸš€ Episode 6200/40000, Epsilon: 0.733\n",
      "Average Reward: -11634.08\n",
      "Average passenger: 493.90\n",
      "Steps to end: 4058.86\n",
      "ðŸš€ Episode 6300/40000, Epsilon: 0.730\n",
      "Average Reward: -10992.78\n",
      "Average passenger: 608.24\n",
      "Steps to end: 3918.30\n",
      "ðŸš€ Episode 6400/40000, Epsilon: 0.726\n",
      "Average Reward: -12081.88\n",
      "Average passenger: 560.74\n",
      "Steps to end: 4247.80\n",
      "ðŸš€ Episode 6500/40000, Epsilon: 0.723\n",
      "Average Reward: -11192.86\n",
      "Average passenger: 543.64\n",
      "Steps to end: 3839.52\n",
      "ðŸš€ Episode 6600/40000, Epsilon: 0.719\n",
      "Average Reward: -10619.64\n",
      "Average passenger: 484.98\n",
      "Steps to end: 3766.88\n",
      "ðŸš€ Episode 6700/40000, Epsilon: 0.715\n",
      "Average Reward: -11098.90\n",
      "Average passenger: 383.84\n",
      "Steps to end: 3724.66\n",
      "ðŸš€ Episode 6800/40000, Epsilon: 0.712\n",
      "Average Reward: -9774.13\n",
      "Average passenger: 421.76\n",
      "Steps to end: 3574.78\n",
      "ðŸš€ Episode 6900/40000, Epsilon: 0.708\n",
      "Average Reward: -8436.01\n",
      "Average passenger: 391.64\n",
      "Steps to end: 3125.90\n",
      "ðŸš€ Episode 7000/40000, Epsilon: 0.705\n",
      "Average Reward: -8938.79\n",
      "Average passenger: 589.40\n",
      "Steps to end: 3260.32\n",
      "ðŸš€ Episode 7100/40000, Epsilon: 0.701\n",
      "Average Reward: -9410.54\n",
      "Average passenger: 460.56\n",
      "Steps to end: 3503.70\n",
      "ðŸš€ Episode 7200/40000, Epsilon: 0.698\n",
      "Average Reward: -10295.57\n",
      "Average passenger: 392.78\n",
      "Steps to end: 3739.60\n",
      "ðŸš€ Episode 7300/40000, Epsilon: 0.694\n",
      "Average Reward: -10152.21\n",
      "Average passenger: 598.48\n",
      "Steps to end: 3729.98\n",
      "ðŸš€ Episode 7400/40000, Epsilon: 0.691\n",
      "Average Reward: -9762.11\n",
      "Average passenger: 550.42\n",
      "Steps to end: 3609.32\n",
      "ðŸš€ Episode 7500/40000, Epsilon: 0.687\n",
      "Average Reward: -11686.40\n",
      "Average passenger: 547.50\n",
      "Steps to end: 4308.16\n",
      "ðŸš€ Episode 7600/40000, Epsilon: 0.684\n",
      "Average Reward: -9454.67\n",
      "Average passenger: 563.40\n",
      "Steps to end: 3649.28\n",
      "ðŸš€ Episode 7700/40000, Epsilon: 0.680\n",
      "Average Reward: -9067.31\n",
      "Average passenger: 413.78\n",
      "Steps to end: 3357.54\n",
      "ðŸš€ Episode 7800/40000, Epsilon: 0.677\n",
      "Average Reward: -9567.68\n",
      "Average passenger: 429.20\n",
      "Steps to end: 3699.32\n",
      "ðŸš€ Episode 7900/40000, Epsilon: 0.674\n",
      "Average Reward: -8259.63\n",
      "Average passenger: 321.74\n",
      "Steps to end: 3283.84\n",
      "ðŸš€ Episode 8000/40000, Epsilon: 0.670\n",
      "Average Reward: -9591.78\n",
      "Average passenger: 583.98\n",
      "Steps to end: 3698.24\n",
      "ðŸš€ Episode 8100/40000, Epsilon: 0.667\n",
      "Average Reward: -9947.92\n",
      "Average passenger: 527.12\n",
      "Steps to end: 3924.60\n",
      "ðŸš€ Episode 8200/40000, Epsilon: 0.664\n",
      "Average Reward: -9891.07\n",
      "Average passenger: 510.80\n",
      "Steps to end: 3777.48\n",
      "ðŸš€ Episode 8300/40000, Epsilon: 0.660\n",
      "Average Reward: -7620.85\n",
      "Average passenger: 408.78\n",
      "Steps to end: 3087.94\n",
      "ðŸš€ Episode 8400/40000, Epsilon: 0.657\n",
      "Average Reward: -8864.82\n",
      "Average passenger: 515.02\n",
      "Steps to end: 3576.54\n",
      "ðŸš€ Episode 8500/40000, Epsilon: 0.654\n",
      "Average Reward: -8242.20\n",
      "Average passenger: 343.62\n",
      "Steps to end: 3380.52\n",
      "ðŸš€ Episode 8600/40000, Epsilon: 0.651\n",
      "Average Reward: -8226.02\n",
      "Average passenger: 386.68\n",
      "Steps to end: 3387.70\n",
      "ðŸš€ Episode 8700/40000, Epsilon: 0.647\n",
      "Average Reward: -7640.64\n",
      "Average passenger: 240.76\n",
      "Steps to end: 3161.96\n",
      "ðŸš€ Episode 8800/40000, Epsilon: 0.644\n",
      "Average Reward: -10166.02\n",
      "Average passenger: 494.36\n",
      "Steps to end: 3934.64\n",
      "ðŸš€ Episode 8900/40000, Epsilon: 0.641\n",
      "Average Reward: -8425.79\n",
      "Average passenger: 483.46\n",
      "Steps to end: 3485.52\n",
      "ðŸš€ Episode 9000/40000, Epsilon: 0.638\n",
      "Average Reward: -7419.26\n",
      "Average passenger: 301.94\n",
      "Steps to end: 3126.94\n",
      "ðŸš€ Episode 9100/40000, Epsilon: 0.634\n",
      "Average Reward: -9762.81\n",
      "Average passenger: 597.30\n",
      "Steps to end: 4083.04\n",
      "ðŸš€ Episode 9200/40000, Epsilon: 0.631\n",
      "Average Reward: -7188.82\n",
      "Average passenger: 513.54\n",
      "Steps to end: 3111.38\n",
      "ðŸš€ Episode 9300/40000, Epsilon: 0.628\n",
      "Average Reward: -6176.60\n",
      "Average passenger: 514.36\n",
      "Steps to end: 2757.16\n",
      "ðŸš€ Episode 9400/40000, Epsilon: 0.625\n",
      "Average Reward: -7666.96\n",
      "Average passenger: 443.68\n",
      "Steps to end: 3241.52\n",
      "ðŸš€ Episode 9500/40000, Epsilon: 0.622\n",
      "Average Reward: -6879.88\n",
      "Average passenger: 559.10\n",
      "Steps to end: 2967.72\n",
      "ðŸš€ Episode 9600/40000, Epsilon: 0.619\n",
      "Average Reward: -7506.94\n",
      "Average passenger: 174.76\n",
      "Steps to end: 3092.70\n",
      "ðŸš€ Episode 9700/40000, Epsilon: 0.616\n",
      "Average Reward: -7152.10\n",
      "Average passenger: 424.96\n",
      "Steps to end: 3153.10\n",
      "ðŸš€ Episode 9800/40000, Epsilon: 0.613\n",
      "Average Reward: -5952.97\n",
      "Average passenger: 543.66\n",
      "Steps to end: 2691.06\n",
      "ðŸš€ Episode 9900/40000, Epsilon: 0.610\n",
      "Average Reward: -5896.27\n",
      "Average passenger: 265.16\n",
      "Steps to end: 2693.92\n",
      "ðŸš€ Episode 10000/40000, Epsilon: 0.607\n",
      "Average Reward: -5694.97\n",
      "Average passenger: 479.04\n",
      "Steps to end: 2612.46\n",
      "ðŸš€ Episode 10100/40000, Epsilon: 0.603\n",
      "Average Reward: -5453.64\n",
      "Average passenger: 287.04\n",
      "Steps to end: 2533.42\n",
      "ðŸš€ Episode 10200/40000, Epsilon: 0.600\n",
      "Average Reward: -7049.98\n",
      "Average passenger: 506.26\n",
      "Steps to end: 3144.70\n",
      "ðŸš€ Episode 10300/40000, Epsilon: 0.597\n",
      "Average Reward: -6171.19\n",
      "Average passenger: 414.40\n",
      "Steps to end: 2799.86\n",
      "ðŸš€ Episode 10400/40000, Epsilon: 0.595\n",
      "Average Reward: -5672.83\n",
      "Average passenger: 524.98\n",
      "Steps to end: 2706.34\n",
      "ðŸš€ Episode 10500/40000, Epsilon: 0.592\n",
      "Average Reward: -6333.91\n",
      "Average passenger: 535.34\n",
      "Steps to end: 2957.26\n",
      "ðŸš€ Episode 10600/40000, Epsilon: 0.589\n",
      "Average Reward: -5173.70\n",
      "Average passenger: 479.30\n",
      "Steps to end: 2483.16\n",
      "ðŸš€ Episode 10700/40000, Epsilon: 0.586\n",
      "Average Reward: -6829.42\n",
      "Average passenger: 404.64\n",
      "Steps to end: 3184.02\n",
      "ðŸš€ Episode 10800/40000, Epsilon: 0.583\n",
      "Average Reward: -7111.80\n",
      "Average passenger: 490.02\n",
      "Steps to end: 3315.82\n",
      "ðŸš€ Episode 10900/40000, Epsilon: 0.580\n",
      "Average Reward: -5430.67\n",
      "Average passenger: 247.58\n",
      "Steps to end: 2504.54\n",
      "ðŸš€ Episode 11000/40000, Epsilon: 0.577\n",
      "Average Reward: -5199.74\n",
      "Average passenger: 262.34\n",
      "Steps to end: 2566.16\n",
      "ðŸš€ Episode 11100/40000, Epsilon: 0.574\n",
      "Average Reward: -6752.08\n",
      "Average passenger: 303.64\n",
      "Steps to end: 3309.34\n",
      "ðŸš€ Episode 11200/40000, Epsilon: 0.571\n",
      "Average Reward: -5893.55\n",
      "Average passenger: 329.32\n",
      "Steps to end: 2868.78\n",
      "ðŸš€ Episode 11300/40000, Epsilon: 0.568\n",
      "Average Reward: -4657.28\n",
      "Average passenger: 299.92\n",
      "Steps to end: 2362.40\n",
      "ðŸš€ Episode 11400/40000, Epsilon: 0.566\n",
      "Average Reward: -5949.58\n",
      "Average passenger: 349.14\n",
      "Steps to end: 2869.50\n",
      "ðŸš€ Episode 11500/40000, Epsilon: 0.563\n",
      "Average Reward: -5676.55\n",
      "Average passenger: 479.86\n",
      "Steps to end: 2809.22\n",
      "ðŸš€ Episode 11600/40000, Epsilon: 0.560\n",
      "Average Reward: -6838.59\n",
      "Average passenger: 470.60\n",
      "Steps to end: 3325.24\n",
      "ðŸš€ Episode 11700/40000, Epsilon: 0.557\n",
      "Average Reward: -5222.62\n",
      "Average passenger: 367.44\n",
      "Steps to end: 2551.18\n",
      "ðŸš€ Episode 11800/40000, Epsilon: 0.554\n",
      "Average Reward: -4498.29\n",
      "Average passenger: 280.32\n",
      "Steps to end: 2374.30\n",
      "ðŸš€ Episode 11900/40000, Epsilon: 0.552\n",
      "Average Reward: -6285.54\n",
      "Average passenger: 305.52\n",
      "Steps to end: 3035.22\n",
      "ðŸš€ Episode 12000/40000, Epsilon: 0.549\n",
      "Average Reward: -4608.23\n",
      "Average passenger: 315.66\n",
      "Steps to end: 2467.00\n",
      "ðŸš€ Episode 12100/40000, Epsilon: 0.546\n",
      "Average Reward: -4454.27\n",
      "Average passenger: 243.46\n",
      "Steps to end: 2408.86\n",
      "ðŸš€ Episode 12200/40000, Epsilon: 0.543\n",
      "Average Reward: -5859.69\n",
      "Average passenger: 331.68\n",
      "Steps to end: 2637.22\n",
      "ðŸš€ Episode 12300/40000, Epsilon: 0.541\n",
      "Average Reward: -4922.14\n",
      "Average passenger: 340.00\n",
      "Steps to end: 2509.50\n",
      "ðŸš€ Episode 12400/40000, Epsilon: 0.538\n",
      "Average Reward: -4637.34\n",
      "Average passenger: 310.22\n",
      "Steps to end: 2512.84\n",
      "ðŸš€ Episode 12500/40000, Epsilon: 0.535\n",
      "Average Reward: -6043.12\n",
      "Average passenger: 571.56\n",
      "Steps to end: 3032.10\n",
      "ðŸš€ Episode 12600/40000, Epsilon: 0.533\n",
      "Average Reward: -5169.61\n",
      "Average passenger: 531.14\n",
      "Steps to end: 2703.96\n",
      "ðŸš€ Episode 12700/40000, Epsilon: 0.530\n",
      "Average Reward: -5413.57\n",
      "Average passenger: 299.76\n",
      "Steps to end: 2863.32\n",
      "ðŸš€ Episode 12800/40000, Epsilon: 0.527\n",
      "Average Reward: -5392.19\n",
      "Average passenger: 429.64\n",
      "Steps to end: 2650.72\n",
      "ðŸš€ Episode 12900/40000, Epsilon: 0.525\n",
      "Average Reward: -3713.89\n",
      "Average passenger: 268.40\n",
      "Steps to end: 2120.76\n",
      "ðŸš€ Episode 13000/40000, Epsilon: 0.522\n",
      "Average Reward: -5634.77\n",
      "Average passenger: 490.88\n",
      "Steps to end: 2637.40\n",
      "ðŸš€ Episode 13100/40000, Epsilon: 0.519\n",
      "Average Reward: -3458.35\n",
      "Average passenger: 377.72\n",
      "Steps to end: 2001.96\n",
      "ðŸš€ Episode 13200/40000, Epsilon: 0.517\n",
      "Average Reward: -3591.61\n",
      "Average passenger: 195.66\n",
      "Steps to end: 2126.50\n",
      "ðŸš€ Episode 13300/40000, Epsilon: 0.514\n",
      "Average Reward: -4052.91\n",
      "Average passenger: 314.28\n",
      "Steps to end: 2435.60\n",
      "ðŸš€ Episode 13400/40000, Epsilon: 0.512\n",
      "Average Reward: -3886.47\n",
      "Average passenger: 336.68\n",
      "Steps to end: 2296.64\n",
      "ðŸš€ Episode 13500/40000, Epsilon: 0.509\n",
      "Average Reward: -4374.68\n",
      "Average passenger: 258.94\n",
      "Steps to end: 2553.42\n",
      "ðŸš€ Episode 13600/40000, Epsilon: 0.507\n",
      "Average Reward: -4867.97\n",
      "Average passenger: 345.28\n",
      "Steps to end: 2851.14\n",
      "ðŸš€ Episode 13700/40000, Epsilon: 0.504\n",
      "Average Reward: -4156.47\n",
      "Average passenger: 384.20\n",
      "Steps to end: 2517.90\n",
      "ðŸš€ Episode 13800/40000, Epsilon: 0.502\n",
      "Average Reward: -4602.30\n",
      "Average passenger: 431.68\n",
      "Steps to end: 2747.58\n",
      "ðŸš€ Episode 13900/40000, Epsilon: 0.499\n",
      "Average Reward: -5517.48\n",
      "Average passenger: 503.14\n",
      "Steps to end: 3298.50\n",
      "ðŸš€ Episode 14000/40000, Epsilon: 0.497\n",
      "Average Reward: -3988.64\n",
      "Average passenger: 514.50\n",
      "Steps to end: 2465.72\n",
      "ðŸš€ Episode 14100/40000, Epsilon: 0.494\n",
      "Average Reward: -4264.93\n",
      "Average passenger: 394.58\n",
      "Steps to end: 2490.16\n",
      "ðŸš€ Episode 14200/40000, Epsilon: 0.492\n",
      "Average Reward: -4524.33\n",
      "Average passenger: 385.68\n",
      "Steps to end: 2771.78\n",
      "ðŸš€ Episode 14300/40000, Epsilon: 0.489\n",
      "Average Reward: -4291.93\n",
      "Average passenger: 314.92\n",
      "Steps to end: 2625.42\n",
      "ðŸš€ Episode 14400/40000, Epsilon: 0.487\n",
      "Average Reward: -4197.35\n",
      "Average passenger: 337.08\n",
      "Steps to end: 2456.20\n",
      "ðŸš€ Episode 14500/40000, Epsilon: 0.484\n",
      "Average Reward: -3408.81\n",
      "Average passenger: 230.68\n",
      "Steps to end: 2175.14\n",
      "ðŸš€ Episode 14600/40000, Epsilon: 0.482\n",
      "Average Reward: -3371.47\n",
      "Average passenger: 213.80\n",
      "Steps to end: 2172.90\n",
      "ðŸš€ Episode 14700/40000, Epsilon: 0.479\n",
      "Average Reward: -4403.80\n",
      "Average passenger: 151.50\n",
      "Steps to end: 2683.44\n",
      "ðŸš€ Episode 14800/40000, Epsilon: 0.477\n",
      "Average Reward: -4732.78\n",
      "Average passenger: 268.22\n",
      "Steps to end: 2975.66\n",
      "ðŸš€ Episode 14900/40000, Epsilon: 0.475\n",
      "Average Reward: -3397.23\n",
      "Average passenger: 226.36\n",
      "Steps to end: 2328.18\n",
      "ðŸš€ Episode 15000/40000, Epsilon: 0.472\n",
      "Average Reward: -3870.52\n",
      "Average passenger: 213.32\n",
      "Steps to end: 2638.48\n",
      "ðŸš€ Episode 15100/40000, Epsilon: 0.470\n",
      "Average Reward: -4689.36\n",
      "Average passenger: 493.52\n",
      "Steps to end: 3015.50\n",
      "ðŸš€ Episode 15200/40000, Epsilon: 0.468\n",
      "Average Reward: -4147.08\n",
      "Average passenger: 381.14\n",
      "Steps to end: 2791.54\n",
      "ðŸš€ Episode 15300/40000, Epsilon: 0.465\n",
      "Average Reward: -4181.29\n",
      "Average passenger: 293.74\n",
      "Steps to end: 2788.94\n",
      "ðŸš€ Episode 15400/40000, Epsilon: 0.463\n",
      "Average Reward: -3538.31\n",
      "Average passenger: 360.16\n",
      "Steps to end: 2202.06\n",
      "ðŸš€ Episode 15500/40000, Epsilon: 0.461\n",
      "Average Reward: -3627.33\n",
      "Average passenger: 275.94\n",
      "Steps to end: 2434.38\n",
      "ðŸš€ Episode 15600/40000, Epsilon: 0.458\n",
      "Average Reward: -3939.25\n",
      "Average passenger: 277.10\n",
      "Steps to end: 2745.50\n",
      "ðŸš€ Episode 15700/40000, Epsilon: 0.456\n",
      "Average Reward: -4964.35\n",
      "Average passenger: 394.10\n",
      "Steps to end: 3230.10\n",
      "ðŸš€ Episode 15800/40000, Epsilon: 0.454\n",
      "Average Reward: -4415.39\n",
      "Average passenger: 381.14\n",
      "Steps to end: 2770.38\n",
      "ðŸš€ Episode 15900/40000, Epsilon: 0.452\n",
      "Average Reward: -3183.41\n",
      "Average passenger: 266.02\n",
      "Steps to end: 2349.86\n",
      "ðŸš€ Episode 16000/40000, Epsilon: 0.449\n",
      "Average Reward: -4097.74\n",
      "Average passenger: 499.64\n",
      "Steps to end: 2920.26\n",
      "ðŸš€ Episode 16100/40000, Epsilon: 0.447\n",
      "Average Reward: -3828.69\n",
      "Average passenger: 423.40\n",
      "Steps to end: 2456.88\n",
      "ðŸš€ Episode 16200/40000, Epsilon: 0.445\n",
      "Average Reward: -3082.68\n",
      "Average passenger: 426.16\n",
      "Steps to end: 2213.82\n",
      "ðŸš€ Episode 16300/40000, Epsilon: 0.443\n",
      "Average Reward: -3516.22\n",
      "Average passenger: 465.20\n",
      "Steps to end: 2650.16\n",
      "ðŸš€ Episode 16400/40000, Epsilon: 0.440\n",
      "Average Reward: -2303.13\n",
      "Average passenger: 366.70\n",
      "Steps to end: 1780.84\n",
      "ðŸš€ Episode 16500/40000, Epsilon: 0.438\n",
      "Average Reward: -3612.85\n",
      "Average passenger: 264.50\n",
      "Steps to end: 2659.26\n",
      "ðŸš€ Episode 16600/40000, Epsilon: 0.436\n",
      "Average Reward: -2797.69\n",
      "Average passenger: 409.94\n",
      "Steps to end: 2062.64\n",
      "ðŸš€ Episode 16700/40000, Epsilon: 0.434\n",
      "Average Reward: -2658.88\n",
      "Average passenger: 330.22\n",
      "Steps to end: 2058.14\n",
      "ðŸš€ Episode 16800/40000, Epsilon: 0.432\n",
      "Average Reward: -2994.62\n",
      "Average passenger: 267.66\n",
      "Steps to end: 2282.88\n",
      "ðŸš€ Episode 16900/40000, Epsilon: 0.430\n",
      "Average Reward: -2535.19\n",
      "Average passenger: 272.52\n",
      "Steps to end: 2004.88\n",
      "ðŸš€ Episode 17000/40000, Epsilon: 0.427\n",
      "Average Reward: -2986.79\n",
      "Average passenger: 251.84\n",
      "Steps to end: 2348.80\n",
      "ðŸš€ Episode 17100/40000, Epsilon: 0.425\n",
      "Average Reward: -4386.09\n",
      "Average passenger: 269.96\n",
      "Steps to end: 3265.48\n",
      "ðŸš€ Episode 17200/40000, Epsilon: 0.423\n",
      "Average Reward: -3080.38\n",
      "Average passenger: 225.76\n",
      "Steps to end: 2452.98\n",
      "ðŸš€ Episode 17300/40000, Epsilon: 0.421\n",
      "Average Reward: -3748.40\n",
      "Average passenger: 324.20\n",
      "Steps to end: 2715.30\n",
      "ðŸš€ Episode 17400/40000, Epsilon: 0.419\n",
      "Average Reward: -3116.44\n",
      "Average passenger: 396.30\n",
      "Steps to end: 2423.90\n",
      "ðŸš€ Episode 17500/40000, Epsilon: 0.417\n",
      "Average Reward: -4473.20\n",
      "Average passenger: 264.42\n",
      "Steps to end: 3158.22\n",
      "ðŸš€ Episode 17600/40000, Epsilon: 0.415\n",
      "Average Reward: -3248.02\n",
      "Average passenger: 203.96\n",
      "Steps to end: 2384.82\n",
      "ðŸš€ Episode 17700/40000, Epsilon: 0.413\n",
      "Average Reward: -2993.44\n",
      "Average passenger: 226.10\n",
      "Steps to end: 2507.14\n",
      "ðŸš€ Episode 17800/40000, Epsilon: 0.411\n",
      "Average Reward: -4469.74\n",
      "Average passenger: 268.64\n",
      "Steps to end: 3052.98\n",
      "ðŸš€ Episode 17900/40000, Epsilon: 0.409\n",
      "Average Reward: -3078.50\n",
      "Average passenger: 216.96\n",
      "Steps to end: 2488.64\n",
      "ðŸš€ Episode 18000/40000, Epsilon: 0.407\n",
      "Average Reward: -2807.79\n",
      "Average passenger: 305.32\n",
      "Steps to end: 2392.46\n",
      "ðŸš€ Episode 18100/40000, Epsilon: 0.405\n",
      "Average Reward: -2986.26\n",
      "Average passenger: 328.34\n",
      "Steps to end: 2543.32\n",
      "ðŸš€ Episode 18200/40000, Epsilon: 0.403\n",
      "Average Reward: -2899.87\n",
      "Average passenger: 161.92\n",
      "Steps to end: 2403.12\n",
      "ðŸš€ Episode 18300/40000, Epsilon: 0.401\n",
      "Average Reward: -3148.58\n",
      "Average passenger: 200.64\n",
      "Steps to end: 2660.50\n",
      "ðŸš€ Episode 18400/40000, Epsilon: 0.399\n",
      "Average Reward: -2863.34\n",
      "Average passenger: 355.78\n",
      "Steps to end: 2265.52\n",
      "ðŸš€ Episode 18500/40000, Epsilon: 0.397\n",
      "Average Reward: -3176.82\n",
      "Average passenger: 243.12\n",
      "Steps to end: 2702.82\n",
      "ðŸš€ Episode 18600/40000, Epsilon: 0.395\n",
      "Average Reward: -2893.65\n",
      "Average passenger: 447.00\n",
      "Steps to end: 2529.90\n",
      "ðŸš€ Episode 18700/40000, Epsilon: 0.393\n",
      "Average Reward: -2607.07\n",
      "Average passenger: 266.30\n",
      "Steps to end: 2360.70\n",
      "ðŸš€ Episode 18800/40000, Epsilon: 0.391\n",
      "Average Reward: -2946.79\n",
      "Average passenger: 191.86\n",
      "Steps to end: 2418.86\n",
      "ðŸš€ Episode 18900/40000, Epsilon: 0.389\n",
      "Average Reward: -2817.56\n",
      "Average passenger: 296.76\n",
      "Steps to end: 2474.44\n",
      "ðŸš€ Episode 19000/40000, Epsilon: 0.387\n",
      "Average Reward: -3256.01\n",
      "Average passenger: 245.32\n",
      "Steps to end: 2789.14\n",
      "ðŸš€ Episode 19100/40000, Epsilon: 0.385\n",
      "Average Reward: -3704.99\n",
      "Average passenger: 214.62\n",
      "Steps to end: 3189.08\n",
      "ðŸš€ Episode 19200/40000, Epsilon: 0.383\n",
      "Average Reward: -2971.64\n",
      "Average passenger: 194.14\n",
      "Steps to end: 2700.70\n",
      "ðŸš€ Episode 19300/40000, Epsilon: 0.381\n",
      "Average Reward: -3091.00\n",
      "Average passenger: 452.72\n",
      "Steps to end: 2704.52\n",
      "ðŸš€ Episode 19400/40000, Epsilon: 0.379\n",
      "Average Reward: -3608.56\n",
      "Average passenger: 270.36\n",
      "Steps to end: 3243.04\n",
      "ðŸš€ Episode 19500/40000, Epsilon: 0.377\n",
      "Average Reward: -2287.03\n",
      "Average passenger: 219.88\n",
      "Steps to end: 2276.12\n",
      "ðŸš€ Episode 19600/40000, Epsilon: 0.375\n",
      "Average Reward: -2801.14\n",
      "Average passenger: 148.22\n",
      "Steps to end: 2733.28\n",
      "ðŸš€ Episode 19700/40000, Epsilon: 0.373\n",
      "Average Reward: -3225.80\n",
      "Average passenger: 315.38\n",
      "Steps to end: 2698.84\n",
      "ðŸš€ Episode 19800/40000, Epsilon: 0.372\n",
      "Average Reward: -1777.96\n",
      "Average passenger: 124.68\n",
      "Steps to end: 1923.86\n",
      "ðŸš€ Episode 19900/40000, Epsilon: 0.370\n",
      "Average Reward: -2640.54\n",
      "Average passenger: 149.40\n",
      "Steps to end: 2449.14\n",
      "ðŸš€ Episode 20000/40000, Epsilon: 0.368\n",
      "Average Reward: -3577.16\n",
      "Average passenger: 110.62\n",
      "Steps to end: 2802.16\n",
      "ðŸš€ Episode 20100/40000, Epsilon: 0.366\n",
      "Average Reward: -4037.78\n",
      "Average passenger: 240.68\n",
      "Steps to end: 3563.66\n",
      "ðŸš€ Episode 20200/40000, Epsilon: 0.364\n",
      "Average Reward: -1640.82\n",
      "Average passenger: 123.74\n",
      "Steps to end: 1819.00\n",
      "ðŸš€ Episode 20300/40000, Epsilon: 0.362\n",
      "Average Reward: -3124.81\n",
      "Average passenger: 103.24\n",
      "Steps to end: 3123.58\n",
      "ðŸš€ Episode 20400/40000, Epsilon: 0.361\n",
      "Average Reward: -1988.64\n",
      "Average passenger: 325.00\n",
      "Steps to end: 2263.92\n",
      "ðŸš€ Episode 20500/40000, Epsilon: 0.359\n",
      "Average Reward: -2751.44\n",
      "Average passenger: 211.70\n",
      "Steps to end: 2721.56\n",
      "ðŸš€ Episode 20600/40000, Epsilon: 0.357\n",
      "Average Reward: -2579.41\n",
      "Average passenger: 238.26\n",
      "Steps to end: 2360.08\n",
      "ðŸš€ Episode 20700/40000, Epsilon: 0.355\n",
      "Average Reward: -2983.89\n",
      "Average passenger: 319.62\n",
      "Steps to end: 2960.24\n",
      "ðŸš€ Episode 20800/40000, Epsilon: 0.353\n",
      "Average Reward: -2075.66\n",
      "Average passenger: 110.42\n",
      "Steps to end: 2281.60\n",
      "ðŸš€ Episode 20900/40000, Epsilon: 0.352\n",
      "Average Reward: -1940.09\n",
      "Average passenger: 255.40\n",
      "Steps to end: 2157.22\n",
      "ðŸš€ Episode 21000/40000, Epsilon: 0.350\n",
      "Average Reward: -2695.30\n",
      "Average passenger: 315.58\n",
      "Steps to end: 2756.68\n",
      "ðŸš€ Episode 21100/40000, Epsilon: 0.348\n",
      "Average Reward: -2369.81\n",
      "Average passenger: 290.24\n",
      "Steps to end: 2481.48\n",
      "ðŸš€ Episode 21200/40000, Epsilon: 0.346\n",
      "Average Reward: -2300.38\n",
      "Average passenger: 119.38\n",
      "Steps to end: 2468.02\n",
      "ðŸš€ Episode 21300/40000, Epsilon: 0.345\n",
      "Average Reward: -2313.56\n",
      "Average passenger: 343.76\n",
      "Steps to end: 2504.84\n",
      "ðŸš€ Episode 21400/40000, Epsilon: 0.343\n",
      "Average Reward: -2756.07\n",
      "Average passenger: 270.26\n",
      "Steps to end: 2968.10\n",
      "ðŸš€ Episode 21500/40000, Epsilon: 0.341\n",
      "Average Reward: -2957.91\n",
      "Average passenger: 302.90\n",
      "Steps to end: 2864.00\n",
      "ðŸš€ Episode 21600/40000, Epsilon: 0.340\n",
      "Average Reward: -2553.22\n",
      "Average passenger: 161.04\n",
      "Steps to end: 2565.14\n",
      "ðŸš€ Episode 21700/40000, Epsilon: 0.338\n",
      "Average Reward: -1601.59\n",
      "Average passenger: 398.56\n",
      "Steps to end: 1837.18\n",
      "ðŸš€ Episode 21800/40000, Epsilon: 0.336\n",
      "Average Reward: -2337.27\n",
      "Average passenger: 188.22\n",
      "Steps to end: 2597.84\n",
      "ðŸš€ Episode 21900/40000, Epsilon: 0.335\n",
      "Average Reward: -2521.81\n",
      "Average passenger: 249.16\n",
      "Steps to end: 2334.76\n",
      "ðŸš€ Episode 22000/40000, Epsilon: 0.333\n",
      "Average Reward: -2618.12\n",
      "Average passenger: 290.40\n",
      "Steps to end: 2935.36\n",
      "ðŸš€ Episode 22100/40000, Epsilon: 0.331\n",
      "Average Reward: -1747.89\n",
      "Average passenger: 138.50\n",
      "Steps to end: 2156.94\n",
      "ðŸš€ Episode 22200/40000, Epsilon: 0.330\n",
      "Average Reward: -2007.43\n",
      "Average passenger: 179.56\n",
      "Steps to end: 2320.70\n",
      "ðŸš€ Episode 22300/40000, Epsilon: 0.328\n",
      "Average Reward: -2880.39\n",
      "Average passenger: 304.32\n",
      "Steps to end: 3284.70\n",
      "ðŸš€ Episode 22400/40000, Epsilon: 0.326\n",
      "Average Reward: -2235.58\n",
      "Average passenger: 269.46\n",
      "Steps to end: 2667.64\n",
      "ðŸš€ Episode 22500/40000, Epsilon: 0.325\n",
      "Average Reward: -2678.15\n",
      "Average passenger: 297.94\n",
      "Steps to end: 3142.40\n",
      "ðŸš€ Episode 22600/40000, Epsilon: 0.323\n",
      "Average Reward: -2107.83\n",
      "Average passenger: 149.84\n",
      "Steps to end: 2562.26\n",
      "ðŸš€ Episode 22700/40000, Epsilon: 0.321\n",
      "Average Reward: -2096.59\n",
      "Average passenger: 187.46\n",
      "Steps to end: 2542.60\n",
      "ðŸš€ Episode 22800/40000, Epsilon: 0.320\n",
      "Average Reward: -1935.38\n",
      "Average passenger: 300.10\n",
      "Steps to end: 2378.82\n",
      "ðŸš€ Episode 22900/40000, Epsilon: 0.318\n",
      "Average Reward: -1594.30\n",
      "Average passenger: 313.10\n",
      "Steps to end: 2036.86\n",
      "ðŸš€ Episode 23000/40000, Epsilon: 0.317\n",
      "Average Reward: -1194.13\n",
      "Average passenger: 114.90\n",
      "Steps to end: 1753.06\n",
      "ðŸš€ Episode 23100/40000, Epsilon: 0.315\n",
      "Average Reward: -1406.40\n",
      "Average passenger: 389.26\n",
      "Steps to end: 1941.16\n",
      "ðŸš€ Episode 23200/40000, Epsilon: 0.313\n",
      "Average Reward: -1760.49\n",
      "Average passenger: 295.32\n",
      "Steps to end: 2028.20\n",
      "ðŸš€ Episode 23300/40000, Epsilon: 0.312\n",
      "Average Reward: -1728.71\n",
      "Average passenger: 235.20\n",
      "Steps to end: 2196.46\n",
      "ðŸš€ Episode 23400/40000, Epsilon: 0.310\n",
      "Average Reward: -1809.91\n",
      "Average passenger: 205.68\n",
      "Steps to end: 2403.94\n",
      "ðŸš€ Episode 23500/40000, Epsilon: 0.309\n",
      "Average Reward: -1698.29\n",
      "Average passenger: 130.26\n",
      "Steps to end: 2462.92\n",
      "ðŸš€ Episode 23600/40000, Epsilon: 0.307\n",
      "Average Reward: -2021.72\n",
      "Average passenger: 313.66\n",
      "Steps to end: 2742.84\n",
      "ðŸš€ Episode 23700/40000, Epsilon: 0.306\n",
      "Average Reward: -1334.59\n",
      "Average passenger: 322.10\n",
      "Steps to end: 1871.96\n",
      "ðŸš€ Episode 23800/40000, Epsilon: 0.304\n",
      "Average Reward: -2328.41\n",
      "Average passenger: 212.90\n",
      "Steps to end: 2945.52\n",
      "ðŸš€ Episode 23900/40000, Epsilon: 0.303\n",
      "Average Reward: -2239.01\n",
      "Average passenger: 105.22\n",
      "Steps to end: 2776.82\n",
      "ðŸš€ Episode 24000/40000, Epsilon: 0.301\n",
      "Average Reward: -1928.67\n",
      "Average passenger: 329.46\n",
      "Steps to end: 2930.78\n",
      "ðŸš€ Episode 24100/40000, Epsilon: 0.300\n",
      "Average Reward: -2975.68\n",
      "Average passenger: 188.34\n",
      "Steps to end: 3429.66\n",
      "ðŸš€ Episode 24200/40000, Epsilon: 0.298\n",
      "Average Reward: -2961.21\n",
      "Average passenger: 181.20\n",
      "Steps to end: 3893.74\n",
      "ðŸš€ Episode 24300/40000, Epsilon: 0.297\n",
      "Average Reward: -1781.10\n",
      "Average passenger: 35.32\n",
      "Steps to end: 2676.60\n",
      "ðŸš€ Episode 24400/40000, Epsilon: 0.295\n",
      "Average Reward: -1536.40\n",
      "Average passenger: 81.74\n",
      "Steps to end: 2325.66\n",
      "ðŸš€ Episode 24500/40000, Epsilon: 0.294\n",
      "Average Reward: -1019.39\n",
      "Average passenger: 51.08\n",
      "Steps to end: 1722.74\n",
      "ðŸš€ Episode 24600/40000, Epsilon: 0.292\n",
      "Average Reward: -1200.17\n",
      "Average passenger: 281.74\n",
      "Steps to end: 2009.74\n",
      "ðŸš€ Episode 24700/40000, Epsilon: 0.291\n",
      "Average Reward: -1361.30\n",
      "Average passenger: 90.14\n",
      "Steps to end: 2388.84\n",
      "ðŸš€ Episode 24800/40000, Epsilon: 0.289\n",
      "Average Reward: -1768.64\n",
      "Average passenger: 158.50\n",
      "Steps to end: 2673.86\n",
      "ðŸš€ Episode 24900/40000, Epsilon: 0.288\n",
      "Average Reward: -1558.95\n",
      "Average passenger: 144.68\n",
      "Steps to end: 2520.12\n",
      "ðŸš€ Episode 25000/40000, Epsilon: 0.286\n",
      "Average Reward: -3085.15\n",
      "Average passenger: 157.82\n",
      "Steps to end: 3560.44\n",
      "ðŸš€ Episode 25100/40000, Epsilon: 0.285\n",
      "Average Reward: -1766.86\n",
      "Average passenger: 108.16\n",
      "Steps to end: 2322.86\n",
      "ðŸš€ Episode 25200/40000, Epsilon: 0.284\n",
      "Average Reward: -2121.90\n",
      "Average passenger: 137.74\n",
      "Steps to end: 3173.26\n",
      "ðŸš€ Episode 25300/40000, Epsilon: 0.282\n",
      "Average Reward: -980.94\n",
      "Average passenger: 49.82\n",
      "Steps to end: 1720.22\n",
      "ðŸš€ Episode 25400/40000, Epsilon: 0.281\n",
      "Average Reward: -1808.94\n",
      "Average passenger: 113.18\n",
      "Steps to end: 2891.56\n",
      "ðŸš€ Episode 25500/40000, Epsilon: 0.279\n",
      "Average Reward: -2027.41\n",
      "Average passenger: 76.14\n",
      "Steps to end: 2942.94\n",
      "ðŸš€ Episode 25600/40000, Epsilon: 0.278\n",
      "Average Reward: -1355.92\n",
      "Average passenger: 228.24\n",
      "Steps to end: 2661.26\n",
      "ðŸš€ Episode 25700/40000, Epsilon: 0.277\n",
      "Average Reward: -1948.70\n",
      "Average passenger: 98.38\n",
      "Steps to end: 2706.28\n",
      "ðŸš€ Episode 25800/40000, Epsilon: 0.275\n",
      "Average Reward: -1939.84\n",
      "Average passenger: 137.00\n",
      "Steps to end: 3143.76\n",
      "ðŸš€ Episode 25900/40000, Epsilon: 0.274\n",
      "Average Reward: -1332.15\n",
      "Average passenger: 101.50\n",
      "Steps to end: 2326.52\n",
      "ðŸš€ Episode 26000/40000, Epsilon: 0.273\n",
      "Average Reward: -1229.03\n",
      "Average passenger: 130.10\n",
      "Steps to end: 2402.56\n",
      "ðŸš€ Episode 26100/40000, Epsilon: 0.271\n",
      "Average Reward: -1592.18\n",
      "Average passenger: 310.44\n",
      "Steps to end: 2775.82\n",
      "ðŸš€ Episode 26200/40000, Epsilon: 0.270\n",
      "Average Reward: -1714.15\n",
      "Average passenger: 91.84\n",
      "Steps to end: 2962.92\n",
      "ðŸš€ Episode 26300/40000, Epsilon: 0.268\n",
      "Average Reward: -1986.09\n",
      "Average passenger: 162.22\n",
      "Steps to end: 3039.38\n",
      "ðŸš€ Episode 26400/40000, Epsilon: 0.267\n",
      "Average Reward: -1644.92\n",
      "Average passenger: 107.30\n",
      "Steps to end: 2770.00\n",
      "ðŸš€ Episode 26500/40000, Epsilon: 0.266\n",
      "Average Reward: -1656.30\n",
      "Average passenger: 155.92\n",
      "Steps to end: 2580.16\n",
      "ðŸš€ Episode 26600/40000, Epsilon: 0.264\n",
      "Average Reward: -937.67\n",
      "Average passenger: 224.74\n",
      "Steps to end: 1920.80\n",
      "ðŸš€ Episode 26700/40000, Epsilon: 0.263\n",
      "Average Reward: -1463.01\n",
      "Average passenger: 160.72\n",
      "Steps to end: 2565.66\n",
      "ðŸš€ Episode 26800/40000, Epsilon: 0.262\n",
      "Average Reward: -1209.02\n",
      "Average passenger: 174.48\n",
      "Steps to end: 2612.22\n",
      "ðŸš€ Episode 26900/40000, Epsilon: 0.261\n",
      "Average Reward: -1266.78\n",
      "Average passenger: 76.52\n",
      "Steps to end: 2193.18\n",
      "ðŸš€ Episode 27000/40000, Epsilon: 0.259\n",
      "Average Reward: -174.26\n",
      "Average passenger: 57.24\n",
      "Steps to end: 1197.12\n",
      "ðŸš€ Episode 27100/40000, Epsilon: 0.258\n",
      "Average Reward: -1507.47\n",
      "Average passenger: 106.58\n",
      "Steps to end: 2732.14\n",
      "ðŸš€ Episode 27200/40000, Epsilon: 0.257\n",
      "Average Reward: -962.82\n",
      "Average passenger: 53.06\n",
      "Steps to end: 2126.02\n",
      "ðŸš€ Episode 27300/40000, Epsilon: 0.255\n",
      "Average Reward: -1300.66\n",
      "Average passenger: 147.96\n",
      "Steps to end: 2381.74\n",
      "ðŸš€ Episode 27400/40000, Epsilon: 0.254\n",
      "Average Reward: -1061.96\n",
      "Average passenger: 261.46\n",
      "Steps to end: 2394.58\n",
      "ðŸš€ Episode 27500/40000, Epsilon: 0.253\n",
      "Average Reward: -938.28\n",
      "Average passenger: 197.74\n",
      "Steps to end: 2199.30\n",
      "ðŸš€ Episode 27600/40000, Epsilon: 0.252\n",
      "Average Reward: -1743.16\n",
      "Average passenger: 142.58\n",
      "Steps to end: 2776.20\n",
      "ðŸš€ Episode 27700/40000, Epsilon: 0.250\n",
      "Average Reward: -737.50\n",
      "Average passenger: 111.94\n",
      "Steps to end: 2015.02\n",
      "ðŸš€ Episode 27800/40000, Epsilon: 0.249\n",
      "Average Reward: -1128.91\n",
      "Average passenger: 141.98\n",
      "Steps to end: 2431.30\n",
      "ðŸš€ Episode 27900/40000, Epsilon: 0.248\n",
      "Average Reward: -1496.62\n",
      "Average passenger: 275.56\n",
      "Steps to end: 2787.76\n",
      "ðŸš€ Episode 28000/40000, Epsilon: 0.247\n",
      "Average Reward: -1146.82\n",
      "Average passenger: 204.42\n",
      "Steps to end: 2566.24\n",
      "ðŸš€ Episode 28100/40000, Epsilon: 0.245\n",
      "Average Reward: -1053.54\n",
      "Average passenger: 147.92\n",
      "Steps to end: 2347.74\n",
      "ðŸš€ Episode 28200/40000, Epsilon: 0.244\n",
      "Average Reward: -1766.95\n",
      "Average passenger: 153.92\n",
      "Steps to end: 3106.96\n",
      "ðŸš€ Episode 28300/40000, Epsilon: 0.243\n",
      "Average Reward: -2174.42\n",
      "Average passenger: 311.40\n",
      "Steps to end: 3722.20\n",
      "ðŸš€ Episode 28400/40000, Epsilon: 0.242\n",
      "Average Reward: -1445.08\n",
      "Average passenger: 285.06\n",
      "Steps to end: 2455.86\n",
      "ðŸš€ Episode 28500/40000, Epsilon: 0.240\n",
      "Average Reward: -919.10\n",
      "Average passenger: 129.84\n",
      "Steps to end: 2383.08\n",
      "ðŸš€ Episode 28600/40000, Epsilon: 0.239\n",
      "Average Reward: -2026.25\n",
      "Average passenger: 192.56\n",
      "Steps to end: 3122.66\n",
      "ðŸš€ Episode 28700/40000, Epsilon: 0.238\n",
      "Average Reward: -748.48\n",
      "Average passenger: 181.50\n",
      "Steps to end: 2117.12\n",
      "ðŸš€ Episode 28800/40000, Epsilon: 0.237\n",
      "Average Reward: -1895.49\n",
      "Average passenger: 95.54\n",
      "Steps to end: 2915.98\n",
      "ðŸš€ Episode 28900/40000, Epsilon: 0.236\n",
      "Average Reward: -1279.28\n",
      "Average passenger: 179.16\n",
      "Steps to end: 3008.10\n",
      "ðŸš€ Episode 29000/40000, Epsilon: 0.235\n",
      "Average Reward: -744.84\n",
      "Average passenger: 144.12\n",
      "Steps to end: 2078.46\n",
      "ðŸš€ Episode 29100/40000, Epsilon: 0.233\n",
      "Average Reward: -1033.94\n",
      "Average passenger: 119.84\n",
      "Steps to end: 2461.18\n",
      "ðŸš€ Episode 29200/40000, Epsilon: 0.232\n",
      "Average Reward: -1676.91\n",
      "Average passenger: 272.00\n",
      "Steps to end: 3691.64\n",
      "ðŸš€ Episode 29300/40000, Epsilon: 0.231\n",
      "Average Reward: -2242.13\n",
      "Average passenger: 264.64\n",
      "Steps to end: 3747.42\n",
      "ðŸš€ Episode 29400/40000, Epsilon: 0.230\n",
      "Average Reward: -1540.62\n",
      "Average passenger: 67.90\n",
      "Steps to end: 3470.30\n",
      "ðŸš€ Episode 29500/40000, Epsilon: 0.229\n",
      "Average Reward: -1575.48\n",
      "Average passenger: 85.58\n",
      "Steps to end: 3831.66\n",
      "ðŸš€ Episode 29600/40000, Epsilon: 0.228\n",
      "Average Reward: -1688.83\n",
      "Average passenger: 113.46\n",
      "Steps to end: 3597.34\n",
      "ðŸš€ Episode 29700/40000, Epsilon: 0.226\n",
      "Average Reward: -1361.79\n",
      "Average passenger: 137.78\n",
      "Steps to end: 3041.62\n",
      "ðŸš€ Episode 29800/40000, Epsilon: 0.225\n",
      "Average Reward: -1332.92\n",
      "Average passenger: 198.22\n",
      "Steps to end: 3420.34\n",
      "ðŸš€ Episode 29900/40000, Epsilon: 0.224\n",
      "Average Reward: -980.63\n",
      "Average passenger: 91.72\n",
      "Steps to end: 2562.64\n",
      "ðŸš€ Episode 30000/40000, Epsilon: 0.223\n",
      "Average Reward: -917.76\n",
      "Average passenger: 64.16\n",
      "Steps to end: 2690.04\n",
      "ðŸš€ Episode 30100/40000, Epsilon: 0.222\n",
      "Average Reward: -1268.33\n",
      "Average passenger: 147.24\n",
      "Steps to end: 3153.56\n",
      "ðŸš€ Episode 30200/40000, Epsilon: 0.221\n",
      "Average Reward: -777.66\n",
      "Average passenger: 72.28\n",
      "Steps to end: 2446.72\n",
      "ðŸš€ Episode 30300/40000, Epsilon: 0.220\n",
      "Average Reward: -1226.80\n",
      "Average passenger: 164.36\n",
      "Steps to end: 3132.14\n",
      "ðŸš€ Episode 30400/40000, Epsilon: 0.219\n",
      "Average Reward: -1586.21\n",
      "Average passenger: 145.98\n",
      "Steps to end: 3287.94\n",
      "ðŸš€ Episode 30500/40000, Epsilon: 0.218\n",
      "Average Reward: -1123.38\n",
      "Average passenger: 93.40\n",
      "Steps to end: 3475.06\n",
      "ðŸš€ Episode 30600/40000, Epsilon: 0.217\n",
      "Average Reward: -1466.64\n",
      "Average passenger: 207.24\n",
      "Steps to end: 3045.42\n",
      "ðŸš€ Episode 30700/40000, Epsilon: 0.215\n",
      "Average Reward: -1102.37\n",
      "Average passenger: 52.60\n",
      "Steps to end: 2752.40\n",
      "ðŸš€ Episode 30800/40000, Epsilon: 0.214\n",
      "Average Reward: -1476.42\n",
      "Average passenger: 306.70\n",
      "Steps to end: 4424.26\n",
      "ðŸš€ Episode 30900/40000, Epsilon: 0.213\n",
      "Average Reward: -1489.55\n",
      "Average passenger: 160.42\n",
      "Steps to end: 3725.26\n",
      "ðŸš€ Episode 31000/40000, Epsilon: 0.212\n",
      "Average Reward: -962.45\n",
      "Average passenger: 78.52\n",
      "Steps to end: 3132.16\n",
      "ðŸš€ Episode 31100/40000, Epsilon: 0.211\n",
      "Average Reward: -897.57\n",
      "Average passenger: 182.76\n",
      "Steps to end: 2612.20\n",
      "ðŸš€ Episode 31200/40000, Epsilon: 0.210\n",
      "Average Reward: -1202.68\n",
      "Average passenger: 60.30\n",
      "Steps to end: 3319.22\n",
      "ðŸš€ Episode 31300/40000, Epsilon: 0.209\n",
      "Average Reward: -2088.32\n",
      "Average passenger: 165.82\n",
      "Steps to end: 4265.20\n",
      "ðŸš€ Episode 31400/40000, Epsilon: 0.208\n",
      "Average Reward: -1281.32\n",
      "Average passenger: 107.70\n",
      "Steps to end: 3635.38\n",
      "ðŸš€ Episode 31500/40000, Epsilon: 0.207\n",
      "Average Reward: -855.82\n",
      "Average passenger: 63.36\n",
      "Steps to end: 3266.36\n",
      "ðŸš€ Episode 31600/40000, Epsilon: 0.206\n",
      "Average Reward: -1519.72\n",
      "Average passenger: 48.78\n",
      "Steps to end: 3054.10\n",
      "ðŸš€ Episode 31700/40000, Epsilon: 0.205\n",
      "Average Reward: -628.12\n",
      "Average passenger: 155.66\n",
      "Steps to end: 2523.14\n",
      "ðŸš€ Episode 31800/40000, Epsilon: 0.204\n",
      "Average Reward: -627.45\n",
      "Average passenger: 74.24\n",
      "Steps to end: 2676.52\n",
      "ðŸš€ Episode 31900/40000, Epsilon: 0.203\n",
      "Average Reward: -1106.15\n",
      "Average passenger: 106.72\n",
      "Steps to end: 3578.58\n",
      "ðŸš€ Episode 32000/40000, Epsilon: 0.202\n",
      "Average Reward: -843.60\n",
      "Average passenger: 211.36\n",
      "Steps to end: 3536.38\n",
      "ðŸš€ Episode 32100/40000, Epsilon: 0.201\n",
      "Average Reward: -778.95\n",
      "Average passenger: 173.96\n",
      "Steps to end: 2865.10\n",
      "ðŸš€ Episode 32200/40000, Epsilon: 0.200\n",
      "Average Reward: -788.17\n",
      "Average passenger: 54.82\n",
      "Steps to end: 2926.00\n",
      "ðŸš€ Episode 32300/40000, Epsilon: 0.199\n",
      "Average Reward: -1192.25\n",
      "Average passenger: 385.74\n",
      "Steps to end: 3797.42\n",
      "ðŸš€ Episode 32400/40000, Epsilon: 0.198\n",
      "Average Reward: -2514.80\n",
      "Average passenger: 108.82\n",
      "Steps to end: 5583.54\n",
      "ðŸš€ Episode 32500/40000, Epsilon: 0.197\n",
      "Average Reward: -1393.04\n",
      "Average passenger: 27.18\n",
      "Steps to end: 4342.92\n",
      "ðŸš€ Episode 32600/40000, Epsilon: 0.196\n",
      "Average Reward: -1679.35\n",
      "Average passenger: 96.32\n",
      "Steps to end: 5016.08\n",
      "ðŸš€ Episode 32700/40000, Epsilon: 0.195\n",
      "Average Reward: -1254.21\n",
      "Average passenger: 48.90\n",
      "Steps to end: 4229.60\n",
      "ðŸš€ Episode 32800/40000, Epsilon: 0.194\n",
      "Average Reward: -715.67\n",
      "Average passenger: 102.88\n",
      "Steps to end: 2713.60\n",
      "ðŸš€ Episode 32900/40000, Epsilon: 0.193\n",
      "Average Reward: -902.99\n",
      "Average passenger: 123.72\n",
      "Steps to end: 2921.96\n",
      "ðŸš€ Episode 33000/40000, Epsilon: 0.192\n",
      "Average Reward: -1005.81\n",
      "Average passenger: 160.86\n",
      "Steps to end: 3633.98\n",
      "ðŸš€ Episode 33100/40000, Epsilon: 0.191\n",
      "Average Reward: -782.99\n",
      "Average passenger: 151.24\n",
      "Steps to end: 2665.86\n",
      "ðŸš€ Episode 33200/40000, Epsilon: 0.190\n",
      "Average Reward: -955.53\n",
      "Average passenger: 146.16\n",
      "Steps to end: 3558.06\n",
      "ðŸš€ Episode 33300/40000, Epsilon: 0.189\n",
      "Average Reward: -1196.48\n",
      "Average passenger: 141.42\n",
      "Steps to end: 3411.46\n",
      "ðŸš€ Episode 33400/40000, Epsilon: 0.188\n",
      "Average Reward: -1222.01\n",
      "Average passenger: 115.80\n",
      "Steps to end: 4053.64\n",
      "ðŸš€ Episode 33500/40000, Epsilon: 0.187\n",
      "Average Reward: -885.43\n",
      "Average passenger: 43.94\n",
      "Steps to end: 3625.60\n",
      "ðŸš€ Episode 33600/40000, Epsilon: 0.186\n",
      "Average Reward: -1008.89\n",
      "Average passenger: 101.72\n",
      "Steps to end: 3940.48\n",
      "ðŸš€ Episode 33700/40000, Epsilon: 0.185\n",
      "Average Reward: -1344.41\n",
      "Average passenger: 84.30\n",
      "Steps to end: 4491.30\n",
      "ðŸš€ Episode 33800/40000, Epsilon: 0.185\n",
      "Average Reward: -235.32\n",
      "Average passenger: 83.90\n",
      "Steps to end: 2594.40\n",
      "ðŸš€ Episode 33900/40000, Epsilon: 0.184\n",
      "Average Reward: -550.05\n",
      "Average passenger: 91.72\n",
      "Steps to end: 3278.42\n",
      "ðŸš€ Episode 34000/40000, Epsilon: 0.183\n",
      "Average Reward: -501.47\n",
      "Average passenger: 84.88\n",
      "Steps to end: 3383.94\n",
      "ðŸš€ Episode 34100/40000, Epsilon: 0.182\n",
      "Average Reward: -1226.24\n",
      "Average passenger: 107.44\n",
      "Steps to end: 3873.96\n",
      "ðŸš€ Episode 34200/40000, Epsilon: 0.181\n",
      "Average Reward: -838.21\n",
      "Average passenger: 264.02\n",
      "Steps to end: 3637.82\n",
      "ðŸš€ Episode 34300/40000, Epsilon: 0.180\n",
      "Average Reward: -1418.68\n",
      "Average passenger: 147.00\n",
      "Steps to end: 3364.76\n",
      "ðŸš€ Episode 34400/40000, Epsilon: 0.179\n",
      "Average Reward: -2011.87\n",
      "Average passenger: 92.44\n",
      "Steps to end: 4774.04\n",
      "ðŸš€ Episode 34500/40000, Epsilon: 0.178\n",
      "Average Reward: -535.58\n",
      "Average passenger: 115.92\n",
      "Steps to end: 2894.62\n",
      "ðŸš€ Episode 34600/40000, Epsilon: 0.177\n",
      "Average Reward: -721.99\n",
      "Average passenger: 125.32\n",
      "Steps to end: 3897.16\n",
      "ðŸš€ Episode 34700/40000, Epsilon: 0.176\n",
      "Average Reward: -637.20\n",
      "Average passenger: 62.64\n",
      "Steps to end: 3534.90\n",
      "ðŸš€ Episode 34800/40000, Epsilon: 0.176\n",
      "Average Reward: -702.58\n",
      "Average passenger: 63.42\n",
      "Steps to end: 3523.28\n",
      "ðŸš€ Episode 34900/40000, Epsilon: 0.175\n",
      "Average Reward: -829.89\n",
      "Average passenger: 128.20\n",
      "Steps to end: 4450.92\n",
      "ðŸš€ Episode 35000/40000, Epsilon: 0.174\n",
      "Average Reward: -515.95\n",
      "Average passenger: 162.74\n",
      "Steps to end: 3278.62\n",
      "ðŸš€ Episode 35100/40000, Epsilon: 0.173\n",
      "Average Reward: -688.73\n",
      "Average passenger: 109.60\n",
      "Steps to end: 4250.78\n",
      "ðŸš€ Episode 35200/40000, Epsilon: 0.172\n",
      "Average Reward: -568.29\n",
      "Average passenger: 162.98\n",
      "Steps to end: 4580.84\n",
      "ðŸš€ Episode 35300/40000, Epsilon: 0.171\n",
      "Average Reward: -352.10\n",
      "Average passenger: 96.72\n",
      "Steps to end: 3281.40\n",
      "ðŸš€ Episode 35400/40000, Epsilon: 0.170\n",
      "Average Reward: -424.41\n",
      "Average passenger: 83.46\n",
      "Steps to end: 4030.28\n",
      "ðŸš€ Episode 35500/40000, Epsilon: 0.169\n",
      "Average Reward: -974.58\n",
      "Average passenger: 57.70\n",
      "Steps to end: 3457.58\n",
      "ðŸš€ Episode 35600/40000, Epsilon: 0.169\n",
      "Average Reward: -1025.41\n",
      "Average passenger: 23.10\n",
      "Steps to end: 5242.42\n",
      "ðŸš€ Episode 35700/40000, Epsilon: 0.168\n",
      "Average Reward: -763.08\n",
      "Average passenger: 170.32\n",
      "Steps to end: 4058.64\n",
      "ðŸš€ Episode 35800/40000, Epsilon: 0.167\n",
      "Average Reward: -743.90\n",
      "Average passenger: 180.92\n",
      "Steps to end: 3412.82\n",
      "ðŸš€ Episode 35900/40000, Epsilon: 0.166\n",
      "Average Reward: -793.82\n",
      "Average passenger: 63.12\n",
      "Steps to end: 4347.64\n",
      "ðŸš€ Episode 36000/40000, Epsilon: 0.165\n",
      "Average Reward: -453.91\n",
      "Average passenger: 38.50\n",
      "Steps to end: 3656.00\n",
      "ðŸš€ Episode 36100/40000, Epsilon: 0.164\n",
      "Average Reward: -238.46\n",
      "Average passenger: 55.06\n",
      "Steps to end: 3133.40\n",
      "ðŸš€ Episode 36200/40000, Epsilon: 0.164\n",
      "Average Reward: -885.33\n",
      "Average passenger: 144.64\n",
      "Steps to end: 4434.78\n",
      "ðŸš€ Episode 36300/40000, Epsilon: 0.163\n",
      "Average Reward: -609.81\n",
      "Average passenger: 83.16\n",
      "Steps to end: 4242.02\n",
      "ðŸš€ Episode 36400/40000, Epsilon: 0.162\n",
      "Average Reward: -225.59\n",
      "Average passenger: 241.62\n",
      "Steps to end: 3792.90\n",
      "ðŸš€ Episode 36500/40000, Epsilon: 0.161\n",
      "Average Reward: -459.43\n",
      "Average passenger: 31.30\n",
      "Steps to end: 3733.10\n",
      "ðŸš€ Episode 36600/40000, Epsilon: 0.160\n",
      "Average Reward: -605.47\n",
      "Average passenger: 95.12\n",
      "Steps to end: 3945.76\n",
      "ðŸš€ Episode 36700/40000, Epsilon: 0.160\n",
      "Average Reward: -208.96\n",
      "Average passenger: 77.06\n",
      "Steps to end: 2912.58\n",
      "ðŸš€ Episode 36800/40000, Epsilon: 0.159\n",
      "Average Reward: -480.73\n",
      "Average passenger: 116.82\n",
      "Steps to end: 3546.20\n",
      "ðŸš€ Episode 36900/40000, Epsilon: 0.158\n",
      "Average Reward: -578.20\n",
      "Average passenger: 22.22\n",
      "Steps to end: 5085.68\n",
      "ðŸš€ Episode 37000/40000, Epsilon: 0.157\n",
      "Average Reward: -569.70\n",
      "Average passenger: 64.12\n",
      "Steps to end: 4538.32\n",
      "ðŸš€ Episode 37100/40000, Epsilon: 0.156\n",
      "Average Reward: -311.14\n",
      "Average passenger: 50.68\n",
      "Steps to end: 4055.72\n",
      "ðŸš€ Episode 37200/40000, Epsilon: 0.156\n",
      "Average Reward: -419.21\n",
      "Average passenger: 50.40\n",
      "Steps to end: 3750.62\n",
      "ðŸš€ Episode 37300/40000, Epsilon: 0.155\n",
      "Average Reward: -752.42\n",
      "Average passenger: 75.78\n",
      "Steps to end: 4286.92\n",
      "ðŸš€ Episode 37400/40000, Epsilon: 0.154\n",
      "Average Reward: -678.54\n",
      "Average passenger: 163.36\n",
      "Steps to end: 4671.48\n",
      "ðŸš€ Episode 37500/40000, Epsilon: 0.153\n",
      "Average Reward: -622.05\n",
      "Average passenger: 86.00\n",
      "Steps to end: 4514.56\n",
      "ðŸš€ Episode 37600/40000, Epsilon: 0.153\n",
      "Average Reward: -97.72\n",
      "Average passenger: 165.30\n",
      "Steps to end: 4400.78\n",
      "ðŸš€ Episode 37700/40000, Epsilon: 0.152\n",
      "Average Reward: -347.02\n",
      "Average passenger: 164.34\n",
      "Steps to end: 4059.74\n",
      "ðŸš€ Episode 37800/40000, Epsilon: 0.151\n",
      "Average Reward: -499.40\n",
      "Average passenger: 202.06\n",
      "Steps to end: 4717.94\n",
      "ðŸš€ Episode 37900/40000, Epsilon: 0.150\n",
      "Average Reward: -102.56\n",
      "Average passenger: 170.62\n",
      "Steps to end: 3485.74\n",
      "ðŸš€ Episode 38000/40000, Epsilon: 0.150\n",
      "Average Reward: -160.72\n",
      "Average passenger: 41.70\n",
      "Steps to end: 3563.06\n",
      "ðŸš€ Episode 38100/40000, Epsilon: 0.149\n",
      "Average Reward: -26.44\n",
      "Average passenger: 28.66\n",
      "Steps to end: 2811.66\n",
      "ðŸš€ Episode 38200/40000, Epsilon: 0.148\n",
      "Average Reward: -362.31\n",
      "Average passenger: 54.02\n",
      "Steps to end: 3763.18\n",
      "ðŸš€ Episode 38300/40000, Epsilon: 0.147\n",
      "Average Reward: -214.61\n",
      "Average passenger: 27.92\n",
      "Steps to end: 3191.10\n",
      "ðŸš€ Episode 38400/40000, Epsilon: 0.147\n",
      "Average Reward: -449.23\n",
      "Average passenger: 157.52\n",
      "Steps to end: 4236.48\n",
      "ðŸš€ Episode 38500/40000, Epsilon: 0.146\n",
      "Average Reward: -102.94\n",
      "Average passenger: 44.32\n",
      "Steps to end: 4161.40\n",
      "ðŸš€ Episode 38600/40000, Epsilon: 0.145\n",
      "Average Reward: -56.01\n",
      "Average passenger: 88.40\n",
      "Steps to end: 3971.66\n",
      "ðŸš€ Episode 38700/40000, Epsilon: 0.144\n",
      "Average Reward: -463.94\n",
      "Average passenger: 41.84\n",
      "Steps to end: 4827.36\n",
      "ðŸš€ Episode 38800/40000, Epsilon: 0.144\n",
      "Average Reward: -560.36\n",
      "Average passenger: 126.38\n",
      "Steps to end: 4670.92\n",
      "ðŸš€ Episode 38900/40000, Epsilon: 0.143\n",
      "Average Reward: -330.12\n",
      "Average passenger: 47.74\n",
      "Steps to end: 3279.62\n",
      "ðŸš€ Episode 39000/40000, Epsilon: 0.142\n",
      "Average Reward: -67.72\n",
      "Average passenger: 124.34\n",
      "Steps to end: 4279.28\n",
      "ðŸš€ Episode 39100/40000, Epsilon: 0.142\n",
      "Average Reward: -76.93\n",
      "Average passenger: 105.00\n",
      "Steps to end: 3363.18\n",
      "ðŸš€ Episode 39200/40000, Epsilon: 0.141\n",
      "Average Reward: -14.31\n",
      "Average passenger: 23.04\n",
      "Steps to end: 2796.70\n",
      "ðŸš€ Episode 39300/40000, Epsilon: 0.140\n",
      "Average Reward: -66.43\n",
      "Average passenger: 66.24\n",
      "Steps to end: 3343.48\n",
      "ðŸš€ Episode 39400/40000, Epsilon: 0.139\n",
      "Average Reward: -2.87\n",
      "Average passenger: 88.10\n",
      "Steps to end: 3417.16\n",
      "ðŸš€ Episode 39500/40000, Epsilon: 0.139\n",
      "Average Reward: 11.22\n",
      "Average passenger: 54.10\n",
      "Steps to end: 3483.62\n",
      "ðŸš€ Episode 39600/40000, Epsilon: 0.138\n",
      "Average Reward: -40.70\n",
      "Average passenger: 81.42\n",
      "Steps to end: 4037.74\n",
      "ðŸš€ Episode 39700/40000, Epsilon: 0.137\n",
      "Average Reward: -122.59\n",
      "Average passenger: 149.70\n",
      "Steps to end: 4474.86\n",
      "ðŸš€ Episode 39800/40000, Epsilon: 0.137\n",
      "Average Reward: -40.93\n",
      "Average passenger: 21.02\n",
      "Steps to end: 4038.06\n",
      "ðŸš€ Episode 39900/40000, Epsilon: 0.136\n",
      "Average Reward: -94.27\n",
      "Average passenger: 48.76\n",
      "Steps to end: 4043.08\n",
      "ðŸš€ Episode 40000/40000, Epsilon: 0.135\n",
      "Average Reward: -441.74\n",
      "Average passenger: 270.10\n",
      "Steps to end: 4559.16\n"
     ]
    }
   ],
   "source": [
    "Q_learning_wrapper.Train(total_episodes = 35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode, steps_to_end = Q_learning_wrapper.Test(total_episodes=100)\n",
    "print(np.mean(rewards_per_episode), np.mean(steps_to_end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
